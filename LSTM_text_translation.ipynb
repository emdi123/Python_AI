{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_text_translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1OzTkg5XCklGmKPIYe_gObwpVz0t6POop",
      "authorship_tag": "ABX9TyPfnZzLMec1IuYBDniOJb50",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emdi123/Python_AI/blob/master/LSTM_text_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC6VZ1o5i2x2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title LSTM for language translation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNyKuznhi9M5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Source: https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MaEtGuhK-jt",
        "colab_type": "code",
        "outputId": "adc737be-8c15-4978-f337-0a1f775e2dca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "#Read the data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "\n",
        "############\n",
        "#Data import\n",
        "############\n",
        "lines=pd.read_table(\"/content/drive/My Drive/AI_projects/Data/deu.txt\", names=['Eng', 'De', 'Junk'])\n",
        "lines = lines[['Eng', 'De']]\n",
        "\n",
        "\n",
        "\n",
        "##########\n",
        "#check data\n",
        "##########\n",
        "lines.head(3)\n",
        "# lines.shape"
      ],
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Eng</th>\n",
              "      <th>De</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.</td>\n",
              "      <td>Geh.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Hallo!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hi.</td>\n",
              "      <td>Grüß Gott!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Eng          De\n",
              "0  Go.        Geh.\n",
              "1  Hi.      Hallo!\n",
              "2  Hi.  Grüß Gott!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZFv97jWUZ2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3_cv_r3jCiO",
        "colab_type": "code",
        "outputId": "264ba1a3-8065-4df9-f5ec-58a65c6aeb9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "\"\"\"\n",
        "The function to process the text for deep learning\n",
        "lines: Pandas dataframe\n",
        "\n",
        "output: Pandas dataframe \n",
        "\"\"\"\n",
        "\n",
        "def process_txt(lines):\n",
        "\n",
        "  #################\n",
        "  #process all data\n",
        "  #################\n",
        "  #lines['Eng'].head(20)\n",
        "\n",
        "  #lowercase all charachters\n",
        "  lines.Eng= lines.Eng.apply(lambda x: x.lower())\n",
        "  lines.De= lines.De.apply(lambda x: x.lower())\n",
        "\n",
        "  #remove quotation marks\n",
        "\n",
        "  lines.Eng= lines.Eng.apply(lambda x: re.sub(\"'\", '', x))\n",
        "  lines.De= lines.De.apply(lambda x: re.sub(\"'\", '', x))\n",
        "\n",
        "  #set of all punctuation marks\n",
        "  exclude = set(string.punctuation)\n",
        "\n",
        "  #remove punctuations from the code\n",
        "\n",
        "  lines.Eng= lines.Eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "  lines.De= lines.De.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "\n",
        "  #remove all numbers from text\n",
        "\n",
        "  lines.Eng= lines.Eng.apply(lambda x: re.sub(\"\\d\", '', x))\n",
        "  lines.De= lines.De.apply(lambda x: re.sub(\"\\d\", '', x))\n",
        "\n",
        "  #remove extra spaces\n",
        "  lines.Eng= lines.Eng.apply(lambda x: x.strip())\n",
        "  lines.De= lines.De.apply(lambda x: x.strip())\n",
        "\n",
        "  #removing additional symbols\n",
        "  lines.Eng=lines.Eng.apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "  lines.De=lines.De.apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "\n",
        "  return lines\n",
        "\n",
        "lines1 = process_txt(lines)\n",
        "\n",
        "'''\n",
        "\n",
        "Last step in the data processing\n",
        "\n",
        "input: a Pandas dataframe\n",
        "\n",
        "'''\n",
        "\n",
        "def final_process(lines):\n",
        "  #Add start and end tokens to the target sequence\n",
        "  lines.De=lines.De.apply(lambda x: 'START_ ' + x+' _END')\n",
        "\n",
        "  #shuffle the data\n",
        "  lines = shuffle(lines)\n",
        "\n",
        "  return lines\n",
        "\n",
        "lines2 = final_process (lines)\n",
        "\n",
        "##########################\n",
        "#Checking data\n",
        "##########################\n",
        "\n",
        "lines.head(20)\n",
        "len(lines.Eng)\n",
        "\n",
        "print(lines.tail())"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                        Eng                                         De\n",
            "41864  tom slept restlessly            START_ tom schlief unruhig _END\n",
            "41865  tom slipped and fell  START_ tom rutschte aus und fiel hin _END\n",
            "41866  tom smiled nervously            START_ tom lächelte nervös _END\n",
            "41867  tom sold his company      START_ tom verkaufte seine firma _END\n",
            "41868  tom sold his company                             START_ to _END\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SS4hQIqn8rH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "05410e4a-48a9-4517-c23f-75904146c2a5"
      },
      "source": [
        "lines.tail(5)"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Eng</th>\n",
              "      <th>De</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>41864</th>\n",
              "      <td>tom slept restlessly</td>\n",
              "      <td>START_ tom schlief unruhig _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41865</th>\n",
              "      <td>tom slipped and fell</td>\n",
              "      <td>START_ tom rutschte aus und fiel hin _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41866</th>\n",
              "      <td>tom smiled nervously</td>\n",
              "      <td>START_ tom lächelte nervös _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41867</th>\n",
              "      <td>tom sold his company</td>\n",
              "      <td>START_ tom verkaufte seine firma _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41868</th>\n",
              "      <td>tom sold his company</td>\n",
              "      <td>START_ to _END</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Eng                                         De\n",
              "41864  tom slept restlessly            START_ tom schlief unruhig _END\n",
              "41865  tom slipped and fell  START_ tom rutschte aus und fiel hin _END\n",
              "41866  tom smiled nervously            START_ tom lächelte nervös _END\n",
              "41867  tom sold his company      START_ tom verkaufte seine firma _END\n",
              "41868  tom sold his company                             START_ to _END"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cidMpN0FRpYV",
        "colab_type": "code",
        "outputId": "b6d9b19f-92c2-4c03-d1d3-8ba7b1d441fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "'''\n",
        "This function creates the language vocabulary\n",
        "input : pandas dataframe thaat is processed for text processing\n",
        "'''\n",
        "def create_vocab(lines): \n",
        "\n",
        "\n",
        "#############################\n",
        "# Information retrival for data\n",
        "############################\n",
        "\n",
        "  # Vocabulary of English\n",
        "  # All unique letters in the english words\n",
        "  # They are in sets because sets cannot contain duplicates\n",
        "\n",
        "  all_eng_words=set()\n",
        "  for eng in lines.Eng:\n",
        "      for word in eng.split():\n",
        "          if word not in all_eng_words:\n",
        "              all_eng_words.add(word)\n",
        "\n",
        "  # Vocabulary of German\n",
        "  all_de_words=set()\n",
        "  for de in lines.De:\n",
        "      for word in de.split():\n",
        "          if word not in all_de_words:\n",
        "              all_de_words.add(word)\n",
        "\n",
        "  return all_eng_words, all_de_words\n",
        "\n",
        "\n",
        "#####################\n",
        "#return function\n",
        "#####################\n",
        "all_eng_words, all_de_words = create_vocab(lines)\n",
        "\n",
        "\n",
        "# Max Length of source sequence \n",
        "lenght_list=[]\n",
        "for l in lines.Eng:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_length_src = np.max(lenght_list)\n",
        "\n",
        "\n",
        "\n",
        "# Max Length of target sequence\n",
        "lenght_list=[]\n",
        "for l in lines.De:\n",
        "    lenght_list.append(len(l.split(' ')))\n",
        "max_length_tar = np.max(lenght_list)\n",
        "print('Max length (charachters) of target sequence is '+ str(max_length_tar))\n",
        "\n",
        "\n",
        "#Sorting the list\n",
        "input_words = sorted(list(all_eng_words))\n",
        "target_words = sorted(list(all_de_words))\n",
        "\n",
        "#Here length is the length of all the words together\n",
        "num_encoder_tokens = len(all_eng_words)\n",
        "num_decoder_tokens = len(all_de_words)\n",
        "\n",
        "print(\"Total number of encorders are %s and total number of decorders are %s \"% (num_encoder_tokens,num_decoder_tokens ))\n",
        "\n",
        "num_decoder_tokens += 1 # For zero padding\n",
        "num_encoder_tokens += 1 # For zero padding\n",
        "\n",
        "#Assigning a digit to each word\n",
        "# Convert it into a dictionary\n",
        "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
        "\n",
        "#reversing the order\n",
        "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
        "\n",
        "#print(\"The lenght of source sequence \" + str(num_encoder_tokens) + '\\n' + \"The length of target sequence \" + str(num_decoder_tokens) )\n"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length (charachters) of target sequence is 17\n",
            "Total number of encorders are 5661 and total number of decorders are 9274 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i5RLG97boIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################\n",
        "#Modeling - making ready with tools\n",
        "####################\n",
        "\n",
        "X, y= lines.Eng, lines.De\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.1)\n",
        "\n",
        "#picke the results to use later\n",
        "\n",
        "X_train.to_pickle(\"/content/drive/My Drive/AI_projects/Models/X_train_translate1.pkl\")\n",
        "X_test.to_pickle(\"/content/drive/My Drive/AI_projects/Models/X_test_translate1.pkl\")\n",
        "\n",
        "\n",
        "#####################\n",
        "###################\n",
        "#DID NOT UNDERSTAND\n",
        "######################\n",
        "\n",
        "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
        "    ''' Generate a batch of data '''\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text.split()):\n",
        "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
        "                for t, word in enumerate(target_text.split()):\n",
        "                    if t<len(target_text.split())-1:\n",
        "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        # Offset by one timestep\n",
        "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtOT_PS7xVPy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "0cd37d14-5264-49e7-a654-54133a1ffa35"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5248           i married tom\n",
              "6464           tom will talk\n",
              "36093    which beer is yours\n",
              "21172      it was depressing\n",
              "28519     tom has a tricycle\n",
              "                ...         \n",
              "5434            im a cripple\n",
              "14445       do you play golf\n",
              "9276          tom needs this\n",
              "24457      you take this one\n",
              "16491        please dont die\n",
              "Name: Eng, Length: 37682, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 300
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd37pGuDpZrK",
        "colab_type": "code",
        "outputId": "85f6f4ae-96e9-4887-87c0-0dc16f7c35aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "################\n",
        "#Create the model\n",
        "#This model converts encorder_input_data and decorder_input_data into \n",
        "#decorder_output_data\n",
        "################\n",
        "\n",
        "\n",
        "latent_dim = 50\n",
        "########################\n",
        "#Setting up the encorder\n",
        "encorder_inputs = Input(shape= (None,))\n",
        "\n",
        "#Here we embedd the input\n",
        "#output dimension is fixed at latent_dim\n",
        "#Here we provide the length of the sequence.\n",
        "enc_emb= Embedding(num_encoder_tokens,latent_dim, mask_zero= True )(encorder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state= True)\n",
        "encorder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encorder_states = [state_h, state_c] # only the states are taken\n",
        "\n",
        "######################\n",
        "#setting up the decorder\n",
        "\n",
        "decorder_inputs = Input(shape= (None,))\n",
        "\n",
        "#Here we embedd the input\n",
        "#output dimension is fixed at latent_dim\n",
        "dec_emb_layer= Embedding(num_decoder_tokens,latent_dim, mask_zero= True )\n",
        "\n",
        "#################\n",
        "################\n",
        "#DIDNOT UMDERSTNAD THIS STEP\n",
        "dec_emb = dec_emb_layer(decorder_inputs)\n",
        "#LSTM function call \n",
        "decoder_lstm = LSTM(latent_dim, return_state= True,return_sequences= True)\n",
        "\n",
        "#DID NOT UNDERSTAND THIS\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
        "                                     initial_state=encorder_states)\n",
        "\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encorder_inputs, decorder_inputs], decoder_outputs)\n",
        "\n",
        "##################\n",
        "#compile the model\n",
        "##################\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "#plot the results\n",
        "\n",
        "plot_model(model, show_shapes=True, to_file= \"/content/drive/My Drive/AI_projects/Figures/lstm_translator.png\")\n",
        "model.summary()"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_35\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_49 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_50 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_26 (Embedding)        (None, None, 50)     283100      input_49[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_27 (Embedding)        (None, None, 50)     463750      input_50[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_26 (LSTM)                  [(None, 50), (None,  20200       embedding_26[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_27 (LSTM)                  [(None, None, 50), ( 20200       embedding_27[0][0]               \n",
            "                                                                 lstm_26[0][1]                    \n",
            "                                                                 lstm_26[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, None, 9275)   473025      lstm_27[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,260,275\n",
            "Trainable params: 1,260,275\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRCe5PN0619u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################\n",
        "#MOdel fitting\n",
        "##########################\n",
        "train_samples = len(X_train) # here training sample size will be bigger than the vocabilary size\n",
        "val_samples = len(X_test)\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "\n",
        "#######################\n",
        "#Run only if needed \n",
        "######################\n",
        "\n",
        "#model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
        "                    # steps_per_epoch = train_samples//batch_size,\n",
        "                    # epochs=epochs,\n",
        "                    # validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    # validation_steps = val_samples//batch_size)\n",
        "#save the weights\n",
        "#model.save_weights(\"/content/drive/My Drive/AI_projects/Models/nmt_weights.h5\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkEPJ4ipKMqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#load the weights if needed\n",
        "model.load_weights(\"/content/drive/My Drive/AI_projects/Models/nmt_weights.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKlfEEPZKBET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################\n",
        "#Setting up the interference\n",
        "##########################\n",
        "# Encode the input sequence to get the \"thought vectors\"\n",
        "encoder_model = Model(encorder_inputs, encorder_states)\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape= (latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\n",
        "\n",
        "#give the new inputs to the decorder layer created above\n",
        "dec_emb2= dec_emb_layer(decorder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state= decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "#Final decorder model\n",
        "# Final decoder model\n",
        "decoder_model = Model([decorder_inputs] + decoder_states_inputs,\n",
        "                      [decoder_outputs2] + decoder_states2\n",
        "                      )\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_9eJr2VBdOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################\n",
        "#Setting up the decorder\n",
        "###########################\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index['START_']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '_END' or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH8THd1eBj_2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f4a2e64f-15f0-4223-bc52-df22f4035548"
      },
      "source": [
        "#####################\n",
        "#Evaluation the results\n",
        "####################\n",
        "\n",
        "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
        "k=-1\n",
        "\n",
        "\n",
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_train[k:k+1].values[0])\n",
        "print('Actual German Translation:', y_train[k:k+1].values[0][6:-4])\n",
        "print('Predicted German Translation:', decoded_sentence[:-4])\n",
        "\n",
        "\n",
        "X_train[k:k+1].values[0]\n"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input English sentence: i married tom\n",
            "Actual German Translation:  ich heiratete tom \n",
            "Predicted German Translation:  ich habe tom gesagt \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i married tom'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHCplzvQ3Pyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title SECOND EXAMPLE\n",
        "\n",
        "#Souurce: https://nextjournal.com/gkoehler/machine-translation-seq2seq-cpu\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLEuafM2EKJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-CzQwSo4J3n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "b8e156e1-0bff-481b-ad38-f5b952bf50da"
      },
      "source": [
        "############\n",
        "#Data import\n",
        "############\n",
        "with open(\"/content/drive/My Drive/AI_projects/Data/deu.txt\", 'r', encoding='utf-8') as f:\n",
        "  lines = f.read().split('\\n')\n",
        "\n",
        "print(len(lines))\n",
        "\n",
        "#splitting into input and target\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "\n",
        "num_samples = 10000\n",
        "\n",
        "#go through line by line, fill in the input sequence , target sequence, input and target  charachters \n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "  input_text, target_text,junk  = line.split('\\t')\n",
        "  target_text = '\\t' + target_text + '\\n'\n",
        "  input_texts.append(input_text)\n",
        "  target_texts.append(target_text)\n",
        "  for char in input_text:\n",
        "    if char not in input_characters:\n",
        "      input_characters.add(char)\n",
        "  for char in target_text:\n",
        "    if char not in target_characters:\n",
        "      target_characters.add(char)\n",
        "\n",
        "\n",
        "#let's take a sample\n",
        "print(input_texts[155], target_texts[155])\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
        "\n",
        "print('##############')\n",
        "print(input_characters)\n",
        "\n",
        "#tokenize our charachters\n",
        "input_token_index = dict(\n",
        "  [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "  [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "\n",
        "#creating numeric data\n",
        "import numpy as np\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "  (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "  dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "  (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "  dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "  (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "  dtype='float32')\n",
        "\n",
        "print(\"\\n Shape of encorder input is \" + str(encoder_input_data.shape))\n",
        "\n",
        "print(\"\\n Shape of decorder input is \" + str(decoder_target_data.shape))\n",
        "\n",
        "#filling in ith actual tokens\n",
        "#we have to offset decoder_target_data by one timestep.\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "  for t, char in enumerate(input_text):\n",
        "    encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "  for t, char in enumerate(target_text):\n",
        "    # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "    decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "    if t > 0:\n",
        "      # decoder_target_data will be ahead by one timestep\n",
        "      # and will not include the start character.\n",
        "      decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41869\n",
            "Go home. \tGeh heim.\n",
            "\n",
            "Number of samples: 10000\n",
            "Number of unique input tokens: 71\n",
            "Number of unique output tokens: 87\n",
            "Max sequence length for inputs: 15\n",
            "Max sequence length for outputs: 45\n",
            "##############\n",
            "[' ', '!', '\"', '$', '%', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "\n",
            " Shape of encorder input is (10000, 15, 71)\n",
            "\n",
            " Shape of decorder input is (10000, 45, 87)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EabfIaTv-7Yf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "dc677b97-fe18-4c59-85e8-a4e5a66eed26"
      },
      "source": [
        "################\n",
        "#Building the model\n",
        "################\n",
        "import keras, tensorflow\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "#hyperparameters\n",
        "#dimensionality for the encoder and decoder LSTM layers have to match\n",
        "\n",
        "#batch_size is the number of samples taken for training step.\n",
        "#For instance, here the network takes 64 samples for the first batch of training\n",
        "batch_size = 64  # batch size for training\n",
        "\n",
        "#one epoch is when the entire dataset is passed one time forward and backard \n",
        "epochs = 100  # number of epochs to train for\n",
        "\n",
        "#\n",
        "latent_dim = 256  # latent dimensionality of the encoding space\n",
        "\n",
        "#model\n",
        "\n",
        "#num_encoder_tokens is the number of unique input characters\n",
        "#shape = (,x) : here x is the dimentionnality of the input, None indicates that batch size is arbitery\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "\n",
        "\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "\n",
        "#model architecture\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "#Use the keras API\n",
        "model = Model(inputs=[encoder_inputs, decoder_inputs], \n",
        "              outputs=decoder_outputs)\n",
        "\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, 71)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, None, 87)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 256), (None, 335872      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 256),  352256      input_2[0][0]                    \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 87)     22359       lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 710,487\n",
            "Trainable params: 710,487\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ0vYJViYVXa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "27413f9d-f9ea-4bf3-828b-8936926af109"
      },
      "source": [
        "help(LSTM)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class LSTM in module keras.layers.recurrent:\n",
            "\n",
            "class LSTM(RNN)\n",
            " |  Long Short-Term Memory layer - Hochreiter 1997.\n",
            " |  \n",
            " |  # Arguments\n",
            " |      units: Positive integer, dimensionality of the output space.\n",
            " |      activation: Activation function to use\n",
            " |          (see [activations](../activations.md)).\n",
            " |          Default: hyperbolic tangent (`tanh`).\n",
            " |          If you pass `None`, no activation is applied\n",
            " |          (ie. \"linear\" activation: `a(x) = x`).\n",
            " |      recurrent_activation: Activation function to use\n",
            " |          for the recurrent step\n",
            " |          (see [activations](../activations.md)).\n",
            " |          Default: sigmoid (`sigmoid`).\n",
            " |          If you pass `None`, no activation is applied\n",
            " |          (ie. \"linear\" activation: `a(x) = x`).\n",
            " |      use_bias: Boolean, whether the layer uses a bias vector.\n",
            " |      kernel_initializer: Initializer for the `kernel` weights matrix,\n",
            " |          used for the linear transformation of the inputs.\n",
            " |          (see [initializers](../initializers.md)).\n",
            " |      recurrent_initializer: Initializer for the `recurrent_kernel`\n",
            " |          weights matrix,\n",
            " |          used for the linear transformation of the recurrent state.\n",
            " |          (see [initializers](../initializers.md)).\n",
            " |      bias_initializer: Initializer for the bias vector\n",
            " |          (see [initializers](../initializers.md)).\n",
            " |      unit_forget_bias: Boolean.\n",
            " |          If True, add 1 to the bias of the forget gate at initialization.\n",
            " |          Setting it to true will also force `bias_initializer=\"zeros\"`.\n",
            " |          This is recommended in [Jozefowicz et al. (2015)](\n",
            " |          http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n",
            " |      kernel_regularizer: Regularizer function applied to\n",
            " |          the `kernel` weights matrix\n",
            " |          (see [regularizer](../regularizers.md)).\n",
            " |      recurrent_regularizer: Regularizer function applied to\n",
            " |          the `recurrent_kernel` weights matrix\n",
            " |          (see [regularizer](../regularizers.md)).\n",
            " |      bias_regularizer: Regularizer function applied to the bias vector\n",
            " |          (see [regularizer](../regularizers.md)).\n",
            " |      activity_regularizer: Regularizer function applied to\n",
            " |          the output of the layer (its \"activation\").\n",
            " |          (see [regularizer](../regularizers.md)).\n",
            " |      kernel_constraint: Constraint function applied to\n",
            " |          the `kernel` weights matrix\n",
            " |          (see [constraints](../constraints.md)).\n",
            " |      recurrent_constraint: Constraint function applied to\n",
            " |          the `recurrent_kernel` weights matrix\n",
            " |          (see [constraints](../constraints.md)).\n",
            " |      bias_constraint: Constraint function applied to the bias vector\n",
            " |          (see [constraints](../constraints.md)).\n",
            " |      dropout: Float between 0 and 1.\n",
            " |          Fraction of the units to drop for\n",
            " |          the linear transformation of the inputs.\n",
            " |      recurrent_dropout: Float between 0 and 1.\n",
            " |          Fraction of the units to drop for\n",
            " |          the linear transformation of the recurrent state.\n",
            " |      implementation: Implementation mode, either 1 or 2.\n",
            " |          Mode 1 will structure its operations as a larger number of\n",
            " |          smaller dot products and additions, whereas mode 2 will\n",
            " |          batch them into fewer, larger operations. These modes will\n",
            " |          have different performance profiles on different hardware and\n",
            " |          for different applications.\n",
            " |      return_sequences: Boolean. Whether to return the last output\n",
            " |          in the output sequence, or the full sequence.\n",
            " |      return_state: Boolean. Whether to return the last state\n",
            " |          in addition to the output. The returned elements of the\n",
            " |          states list are the hidden state and the cell state, respectively.\n",
            " |      go_backwards: Boolean (default False).\n",
            " |          If True, process the input sequence backwards and return the\n",
            " |          reversed sequence.\n",
            " |      stateful: Boolean (default False). If True, the last state\n",
            " |          for each sample at index i in a batch will be used as initial\n",
            " |          state for the sample of index i in the following batch.\n",
            " |      unroll: Boolean (default False).\n",
            " |          If True, the network will be unrolled,\n",
            " |          else a symbolic loop will be used.\n",
            " |          Unrolling can speed-up a RNN,\n",
            " |          although it tends to be more memory-intensive.\n",
            " |          Unrolling is only suitable for short sequences.\n",
            " |  \n",
            " |  # References\n",
            " |      - [Long short-term memory](\n",
            " |        http://www.bioinf.jku.at/publications/older/2604.pdf)\n",
            " |      - [Learning to forget: Continual prediction with LSTM](\n",
            " |        http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n",
            " |      - [Supervised sequence labeling with recurrent neural networks](\n",
            " |        http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
            " |      - [A Theoretically Grounded Application of Dropout in\n",
            " |         Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      LSTM\n",
            " |      RNN\n",
            " |      keras.engine.base_layer.Layer\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, units, activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=2, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  call(self, inputs, mask=None, training=None, initial_state=None)\n",
            " |      This is where the layer's logic lives.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: Input tensor, or list/tuple of input tensors.\n",
            " |          **kwargs: Additional keyword arguments.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A tensor or list/tuple of tensors.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the config of the layer.\n",
            " |      \n",
            " |      A layer config is a Python dictionary (serializable)\n",
            " |      containing the configuration of a layer.\n",
            " |      The same layer can be reinstantiated later\n",
            " |      (without its trained weights) from this configuration.\n",
            " |      \n",
            " |      The config of a layer does not include connectivity\n",
            " |      information, nor the layer class name. These are handled\n",
            " |      by `Network` (one layer of abstraction above).\n",
            " |      \n",
            " |      # Returns\n",
            " |          Python dictionary.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_config(config) from builtins.type\n",
            " |      Creates a layer from its config.\n",
            " |      \n",
            " |      This method is the reverse of `get_config`,\n",
            " |      capable of instantiating the same layer from the config\n",
            " |      dictionary. It does not handle layer connectivity\n",
            " |      (handled by Network), nor weights (handled by `set_weights`).\n",
            " |      \n",
            " |      # Arguments\n",
            " |          config: A Python dictionary, typically the\n",
            " |              output of get_config.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A layer instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  activation\n",
            " |  \n",
            " |  bias_constraint\n",
            " |  \n",
            " |  bias_initializer\n",
            " |  \n",
            " |  bias_regularizer\n",
            " |  \n",
            " |  dropout\n",
            " |  \n",
            " |  implementation\n",
            " |  \n",
            " |  kernel_constraint\n",
            " |  \n",
            " |  kernel_initializer\n",
            " |  \n",
            " |  kernel_regularizer\n",
            " |  \n",
            " |  recurrent_activation\n",
            " |  \n",
            " |  recurrent_constraint\n",
            " |  \n",
            " |  recurrent_dropout\n",
            " |  \n",
            " |  recurrent_initializer\n",
            " |  \n",
            " |  recurrent_regularizer\n",
            " |  \n",
            " |  unit_forget_bias\n",
            " |  \n",
            " |  units\n",
            " |  \n",
            " |  use_bias\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from RNN:\n",
            " |  \n",
            " |  __call__(self, inputs, initial_state=None, constants=None, **kwargs)\n",
            " |      Wrapper around self.call(), for handling internal references.\n",
            " |      \n",
            " |      If a Keras tensor is passed:\n",
            " |          - We call self._add_inbound_node().\n",
            " |          - If necessary, we `build` the layer to match\n",
            " |              the _keras_shape of the input(s).\n",
            " |          - We update the _keras_shape of every input tensor with\n",
            " |              its new shape (obtained via self.compute_output_shape).\n",
            " |              This is done as part of _add_inbound_node().\n",
            " |          - We update the _keras_history of the output tensor(s)\n",
            " |              with the current layer.\n",
            " |              This is done as part of _add_inbound_node().\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: Can be a tensor or list/tuple of tensors.\n",
            " |          **kwargs: Additional keyword arguments to be passed to `call()`.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output of the layer's `call` method.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: in case the layer is missing shape information\n",
            " |              for its `build` call.\n",
            " |  \n",
            " |  build(self, input_shape)\n",
            " |      Creates the layer weights.\n",
            " |      \n",
            " |      Must be implemented on all layers that have weights.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          input_shape: Keras tensor (future input to layer)\n",
            " |              or list/tuple of Keras tensors to reference\n",
            " |              for weight shape computations.\n",
            " |  \n",
            " |  compute_mask(self, inputs, mask)\n",
            " |      Computes an output mask tensor.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: Tensor or list of tensors.\n",
            " |          mask: Tensor or list of tensors.\n",
            " |      \n",
            " |      # Returns\n",
            " |          None or a tensor (or list of tensors,\n",
            " |              one per output tensor of the layer).\n",
            " |  \n",
            " |  compute_output_shape(self, input_shape)\n",
            " |      Computes the output shape of the layer.\n",
            " |      \n",
            " |      Assumes that the layer will be built\n",
            " |      to match that input shape provided.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          input_shape: Shape tuple (tuple of integers)\n",
            " |              or list of shape tuples (one per output tensor of the layer).\n",
            " |              Shape tuples can include None for free dimensions,\n",
            " |              instead of an integer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          An output shape tuple.\n",
            " |  \n",
            " |  get_initial_state(self, inputs)\n",
            " |  \n",
            " |  get_losses_for(self, inputs=None)\n",
            " |  \n",
            " |  reset_states(self, states=None)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from RNN:\n",
            " |  \n",
            " |  losses\n",
            " |  \n",
            " |  non_trainable_weights\n",
            " |  \n",
            " |  states\n",
            " |  \n",
            " |  trainable_weights\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __setattr__(self, name, value)\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  add_loss(self, losses, inputs=None)\n",
            " |      Adds losses to the layer.\n",
            " |      \n",
            " |      The loss may potentially be conditional on some inputs tensors,\n",
            " |      for instance activity losses are conditional on the layer's inputs.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          losses: loss tensor or list of loss tensors\n",
            " |              to add to the layer.\n",
            " |          inputs: input tensor or list of inputs tensors to mark\n",
            " |              the losses as conditional on these inputs.\n",
            " |              If None is passed, the loss is assumed unconditional\n",
            " |              (e.g. L2 weight regularization, which only depends\n",
            " |              on the layer's weights variables, not on any inputs tensors).\n",
            " |  \n",
            " |  add_metric(self, value, name=None)\n",
            " |      Adds metric tensor to the layer.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          value: Metric tensor.\n",
            " |          name: String metric name.\n",
            " |  \n",
            " |  add_update(self, updates, inputs=None)\n",
            " |      Adds updates to the layer.\n",
            " |      \n",
            " |      The updates may potentially be conditional on some inputs tensors,\n",
            " |      for instance batch norm updates are conditional on the layer's inputs.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          updates: update op or list of update ops\n",
            " |              to add to the layer.\n",
            " |          inputs: input tensor or list of inputs tensors to mark\n",
            " |              the updates as conditional on these inputs.\n",
            " |              If None is passed, the updates are assumed unconditional.\n",
            " |  \n",
            " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, constraint=None)\n",
            " |      Adds a weight variable to the layer.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          name: String, the name for the weight variable.\n",
            " |          shape: The shape tuple of the weight.\n",
            " |          dtype: The dtype of the weight.\n",
            " |          initializer: An Initializer instance (callable).\n",
            " |          regularizer: An optional Regularizer instance.\n",
            " |          trainable: A boolean, whether the weight should\n",
            " |              be trained via backprop or not (assuming\n",
            " |              that the layer itself is also trainable).\n",
            " |          constraint: An optional Constraint instance.\n",
            " |      \n",
            " |      # Returns\n",
            " |          The created weight variable.\n",
            " |  \n",
            " |  assert_input_compatibility(self, inputs)\n",
            " |      Checks compatibility between the layer and provided inputs.\n",
            " |      \n",
            " |      This checks that the tensor(s) `input`\n",
            " |      verify the input assumptions of the layer\n",
            " |      (if any). If not, exceptions are raised.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          inputs: input tensor or list of input tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: in case of mismatch between\n",
            " |              the provided inputs and the expectations of the layer.\n",
            " |  \n",
            " |  count_params(self)\n",
            " |      Counts the total number of scalars composing the weights.\n",
            " |      \n",
            " |      # Returns\n",
            " |          An integer count.\n",
            " |      \n",
            " |      # Raises\n",
            " |          RuntimeError: if the layer isn't yet built\n",
            " |              (in which case its weights aren't yet defined).\n",
            " |  \n",
            " |  get_input_at(self, node_index)\n",
            " |      Retrieves the input tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_mask_at(self, node_index)\n",
            " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_input_shape_at(self, node_index)\n",
            " |      Retrieves the input shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple inputs).\n",
            " |  \n",
            " |  get_output_at(self, node_index)\n",
            " |      Retrieves the output tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_mask_at(self, node_index)\n",
            " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A mask tensor\n",
            " |          (or list of tensors if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_output_shape_at(self, node_index)\n",
            " |      Retrieves the output shape(s) of a layer at a given node.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          node_index: Integer, index of the node\n",
            " |              from which to retrieve the attribute.\n",
            " |              E.g. `node_index=0` will correspond to the\n",
            " |              first time the layer was called.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A shape tuple\n",
            " |          (or list of shape tuples if the layer has multiple outputs).\n",
            " |  \n",
            " |  get_updates_for(self, inputs)\n",
            " |  \n",
            " |  get_weights(self)\n",
            " |      Returns the current weights of the layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Weights values as a list of numpy arrays.\n",
            " |  \n",
            " |  set_weights(self, weights)\n",
            " |      Sets the weights of the layer, from Numpy arrays.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          weights: a list of Numpy arrays. The number\n",
            " |              of arrays and their shape must match\n",
            " |              number of the dimensions of the weights\n",
            " |              of the layer (i.e. it should match the\n",
            " |              output of `get_weights`).\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: If the provided weights list does not match the\n",
            " |              layer's specifications.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from keras.engine.base_layer.Layer:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  built\n",
            " |  \n",
            " |  input\n",
            " |      Retrieves the input tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Input tensor or list of input tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_mask\n",
            " |      Retrieves the input mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Input mask tensor (potentially None) or list of input\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  input_shape\n",
            " |      Retrieves the input shape tuple(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Input shape tuple\n",
            " |          (or list of input shape tuples, one tuple per input tensor).\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  metrics\n",
            " |  \n",
            " |  output\n",
            " |      Retrieves the output tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output tensor or list of output tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_mask\n",
            " |      Retrieves the output mask tensor(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has exactly one inbound node,\n",
            " |      i.e. if it is connected to one incoming layer.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output mask tensor (potentially None) or list of output\n",
            " |          mask tensors.\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  output_shape\n",
            " |      Retrieves the output shape tuple(s) of a layer.\n",
            " |      \n",
            " |      Only applicable if the layer has one inbound node,\n",
            " |      or if all inbound nodes have the same output shape.\n",
            " |      \n",
            " |      # Returns\n",
            " |          Output shape tuple\n",
            " |          (or list of input shape tuples, one tuple per output tensor).\n",
            " |      \n",
            " |      # Raises\n",
            " |          AttributeError: if the layer is connected to\n",
            " |          more than one incoming layers.\n",
            " |  \n",
            " |  updates\n",
            " |  \n",
            " |  weights\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68vTquoBAE-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1139d613-261f-4f71-f654-a12e24b9ee4a"
      },
      "source": [
        "#Model fitting\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "model.save_weights(\"/content/drive/My Drive/AI_projects/Models/nmt_weights_secondModel.h5\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 23s 3ms/step - loss: 1.1720 - val_loss: 1.1310\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.9125 - val_loss: 0.9110\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.7661 - val_loss: 0.8114\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.6911 - val_loss: 0.7404\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.6428 - val_loss: 0.7041\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.6050 - val_loss: 0.6732\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.5743 - val_loss: 0.6491\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.5478 - val_loss: 0.6313\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.5245 - val_loss: 0.6069\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.5040 - val_loss: 0.5910\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.4849 - val_loss: 0.5811\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.4672 - val_loss: 0.5747\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.4516 - val_loss: 0.5707\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.4365 - val_loss: 0.5595\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.4225 - val_loss: 0.5490\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.4093 - val_loss: 0.5437\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.3961 - val_loss: 0.5374\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.3846 - val_loss: 0.5337\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.3731 - val_loss: 0.5342\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.3617 - val_loss: 0.5332\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.3515 - val_loss: 0.5285\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.3413 - val_loss: 0.5256\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.3310 - val_loss: 0.5266\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.3221 - val_loss: 0.5284\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.3124 - val_loss: 0.5269\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.3040 - val_loss: 0.5311\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.2960 - val_loss: 0.5356\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.2876 - val_loss: 0.5337\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.2797 - val_loss: 0.5343\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.2725 - val_loss: 0.5353\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.2646 - val_loss: 0.5346\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.2577 - val_loss: 0.5379\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.2509 - val_loss: 0.5389\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.2440 - val_loss: 0.5420\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.2378 - val_loss: 0.5501\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.2320 - val_loss: 0.5501\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.2256 - val_loss: 0.5541\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.2202 - val_loss: 0.5578\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.2143 - val_loss: 0.5582\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.2093 - val_loss: 0.5646\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.2038 - val_loss: 0.5715\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1987 - val_loss: 0.5768\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.1945 - val_loss: 0.5751\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1895 - val_loss: 0.5852\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.1847 - val_loss: 0.5880\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1805 - val_loss: 0.5904\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.1766 - val_loss: 0.5986\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1724 - val_loss: 0.5980\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1689 - val_loss: 0.6004\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1644 - val_loss: 0.6072\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1612 - val_loss: 0.6175\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1573 - val_loss: 0.6174\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1544 - val_loss: 0.6366\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.1508 - val_loss: 0.6299\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1478 - val_loss: 0.6343\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.1451 - val_loss: 0.6374\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1421 - val_loss: 0.6431\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1389 - val_loss: 0.6424\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.1363 - val_loss: 0.6505\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.1334 - val_loss: 0.6551\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.1312 - val_loss: 0.6571\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.1282 - val_loss: 0.6667\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1255 - val_loss: 0.6704\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.1237 - val_loss: 0.6781\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.1210 - val_loss: 0.6777\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1188 - val_loss: 0.6801\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.1166 - val_loss: 0.6915\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1141 - val_loss: 0.6921\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.1122 - val_loss: 0.6993\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1100 - val_loss: 0.6998\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1088 - val_loss: 0.7082\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.1066 - val_loss: 0.7115\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1045 - val_loss: 0.7163\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1032 - val_loss: 0.7212\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.1011 - val_loss: 0.7186\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0994 - val_loss: 0.7260\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0978 - val_loss: 0.7322\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0963 - val_loss: 0.7406\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0946 - val_loss: 0.7391\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0931 - val_loss: 0.7432\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0915 - val_loss: 0.7444\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0903 - val_loss: 0.7514\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0886 - val_loss: 0.7522\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0873 - val_loss: 0.7625\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0862 - val_loss: 0.7629\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0846 - val_loss: 0.7686\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0834 - val_loss: 0.7677\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 21s 3ms/step - loss: 0.0822 - val_loss: 0.7728\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0808 - val_loss: 0.7746\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0799 - val_loss: 0.7869\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0787 - val_loss: 0.7802\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0771 - val_loss: 0.7886\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0761 - val_loss: 0.7981\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0747 - val_loss: 0.7950\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0739 - val_loss: 0.8084\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0730 - val_loss: 0.8111\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0722 - val_loss: 0.8110\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0710 - val_loss: 0.8170\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0696 - val_loss: 0.8173\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 20s 3ms/step - loss: 0.0690 - val_loss: 0.8177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk06GsgLAV-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the weights\n",
        "model.load_weights(\"/content/drive/My Drive/AI_projects/Models/nmt_weights_secondModel.h5\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGjrzNlEAwfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Interface\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "  decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "  [decoder_inputs] + decoder_states_inputs,\n",
        "  [decoder_outputs] + decoder_states)\n",
        "\n",
        "# reverse-lookup token index to turn sequences back to characters\n",
        "reverse_input_char_index = dict(\n",
        "  (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "  (i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUWQUr9LBNq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Decoding the sequence\n",
        "def decode_sequence(input_seq):\n",
        "  # encode the input sequence to get the internal state vectors.\n",
        "  states_value = encoder_model.predict(input_seq)\n",
        "  \n",
        "  # generate empty target sequence of length 1 with only the start character\n",
        "  target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "  target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "  \n",
        "  # output sequence loop\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict(\n",
        "      [target_seq] + states_value)\n",
        "    \n",
        "    # sample a token and add the corresponding character to the \n",
        "    # decoded sequence\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "    decoded_sentence += sampled_char\n",
        "    \n",
        "    # check for the exit condition: either hitting max length\n",
        "    # or predicting the 'stop' character\n",
        "    if (sampled_char == '\\n' or \n",
        "        len(decoded_sentence) > max_decoder_seq_length):\n",
        "      stop_condition = True\n",
        "      \n",
        "    # update the target sequence (length 1).\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "    \n",
        "    # update states\n",
        "    states_value = [h, c]\n",
        "    \n",
        "  return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a4VnGc0EL-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ba7a810a-e9ee-4da8-bbcf-f56286021c50"
      },
      "source": [
        "#test the model \n",
        "input_sentence = \" do you have?\"\n",
        "test_sentence_tokenized = np.zeros(\n",
        "  (1, max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n",
        "for t, char in enumerate(input_sentence):\n",
        "  test_sentence_tokenized[0, t, input_token_index[char]] = 1.\n",
        "print(input_sentence)\n",
        "print(decode_sequence(test_sentence_tokenized))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " do you have ?\n",
            "Ich wurde betrennen.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}