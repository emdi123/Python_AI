{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "There are 0 rows with null values\n",
      " \n",
      " \n",
      "Data split done\n",
      " \n",
      "Cross validation done\n",
      " \n",
      "Model fitting done\n",
      " \n",
      "The best classifier is\n",
      "LDA\n",
      "The accuracy of the model is: \n",
      "100.0\n",
      " \n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0  7]]\n",
      " \n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00        10\n",
      "        1.0       1.00      1.00      1.00        13\n",
      "        2.0       1.00      1.00      1.00         7\n",
      "\n",
      "avg / total       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "#User Input\n",
    "#########################\n",
    "#Provide the training dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "# url ='https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data'\n",
    "\n",
    "#provide the column names. Class column should be the labels\n",
    "#*******Label should be marked as 'label'\n",
    "\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "# names=['age', 'work_class','fnlwt','edu','edu_num','mat_stat','occu','rela','race','sex','capital_gain','capital_los','hr_week','native','label']\n",
    "\n",
    "########################\n",
    "# Load libraries\n",
    "########################\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler #for standardizing data\n",
    "le=LabelEncoder()\n",
    "seed = 10\n",
    "\n",
    "\n",
    "############################\n",
    "# Load dataset\n",
    "############################\n",
    "def load_data(url):\n",
    "     dataset = pd.read_csv(url,names=names ,engine='python')\n",
    "     return dataset\n",
    "\n",
    "#Run the function\n",
    "\n",
    "dataset =load_data(url)\n",
    "# dataset = dataset.iloc[1:100,:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########################\n",
    "#Data Exploration\n",
    "#########################\n",
    "#Numeric exploration\n",
    "\n",
    "def explore(dataset):\n",
    "    #shape\n",
    "    print(\"The dimention of the dataset is: \"+ str(dataset.shape))\n",
    "    print(' ')\n",
    "    #Data type of each class\n",
    "    print(\"The data types of the different classes are\")\n",
    "    print(dataset.dtypes)\n",
    "    print(' ')\n",
    "    \n",
    "   \n",
    "    #Data type counts, datatypes, memory usage, number of elements, if null\n",
    "    print(dataset.info())\n",
    "    print(' ')\n",
    "    \n",
    "    #Correlation between attributes\n",
    "    print(\"The correlation between attributes are\")\n",
    "    pd.set_option('precision', 3)\n",
    "    corr = dataset.corr(method='pearson')\n",
    "    sns.heatmap(corr, xticklabels=corr.columns.values,yticklabels=corr.columns.values)\n",
    "    sns.plt.show()\n",
    "\n",
    "    print(' ')\n",
    "    \n",
    "    #Sqewness of univariate distribution\n",
    "    #Skew refers to a distribution that is assumed Gaussian (normal or bell curve)\n",
    "    #that is shifted or squashed in one direction or another.\n",
    "    print('The Sqew of the attributes from Gaussian are')\n",
    "    skew = dataset.skew()\n",
    "    print(skew)\n",
    "    print(' ')\n",
    "    \n",
    "    #summary statistics\n",
    "    print(\"The summary statistics of the dataset is: \")\n",
    "    print(dataset.describe())\n",
    "    print(' ')\n",
    "    \n",
    "        \n",
    "\n",
    "    #Number of rows for each class. \n",
    "    #This show how many different types of data are there\n",
    "\n",
    "    d=data.groupby(list(data)[-1]).size() # group by last column\n",
    "    print(\"The labels in the data are: \")\n",
    "    \n",
    "    print(\"There are %s labels in the dataset \\n\"% (len(d)))\n",
    "\n",
    "    for i in range(0,len(d)):\n",
    "        print(\"The label %s has %s elements \\n\" %( d.index[i],d[i]))\n",
    "\n",
    "    a,b= d.nlargest(2)\n",
    "    if(int(b*2)<=int(a)):\n",
    "        print(\"**********There is huge difference in the counts between the classes****** \\n\")\n",
    "        print(\"**********Need to fix this before modeling****** \")\n",
    "    else: \n",
    "        print(\"The count of the largest class is %s times the second largest class \\n\" % (a/b)) \n",
    "    \n",
    "\n",
    "    #Top10 rows\n",
    "    print(' ')\n",
    "    \n",
    "    print('The first 10 rows are: ')\n",
    "    print(dataset.head(10))\n",
    "    print(' ')\n",
    "    \n",
    "\n",
    "#Run the function\n",
    "# explore(dataset)\n",
    "\n",
    "#Data Exploration by visualzation\n",
    "\n",
    "def data_vis_all(dataset):\n",
    "    #box_whisker plot\n",
    "    #2 rows and 2 colmns\n",
    "    print(\"Univariate plots and Multivariate plots:\")\n",
    "    dataset.plot(kind='box', subplots=True,figsize= (20,5), layout=(1,4), sharex=False, grid= True,sharey=False)\n",
    "    #histogram\n",
    "    dataset.plot(kind='hist', subplots=True,figsize= (20,5), layout=(1,4),sharex=False, grid= True, sharey=False)\n",
    "    scatter_matrix(dataset,figsize=(10, 10))\n",
    "    plt.show()\n",
    "\n",
    "#Run the function\n",
    "#data_vis_all(data)\n",
    "\n",
    "#plot just one component\n",
    "\n",
    "def data_vis_one(dataset, col1):\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].hist(dataset1.col1, 10, facecolor='red', alpha=0.5, label=\"Give me a label\")\n",
    "    ax[1].hist(dataset1.alcohol, 10, facecolor='white', ec=\"black\", lw=0.5, alpha=0.5, label=\"Give me a label\")\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=0.5, hspace=0.05, wspace=1)\n",
    "    ax[0].set_ylim([0, 1000])\n",
    "    ax[0].set_xlabel(\"This is my xlabel\")\n",
    "    ax[0].set_ylabel(\"Frequency\")\n",
    "    ax[1].set_xlabel(\"This is my Y label\")\n",
    "    ax[1].set_ylabel(\"Frequency\")\n",
    "    #ax[0].legend(loc='best')\n",
    "    #ax[1].legend(loc='best')\n",
    "    fig.suptitle(\"Distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "# data_vis_one(dataset,age)\n",
    "\n",
    "\n",
    "#########################\n",
    "#Data processing\n",
    "#########################\n",
    "\n",
    "#Handing columns with unusual numbers\n",
    "#Here we replace unsual number '0' with NaN\n",
    "# dataset[['col1','col2']]= dataset[['col1','col2']].replace(0, np.NaN)\n",
    "\n",
    "#############\n",
    "#Handling null values\n",
    "nrows =sum(dataset.apply(lambda x: sum(x.isnull().values), axis = 1)>0)\n",
    "print(' ')\n",
    "print(\"There are \"+str(nrows)+ \" rows with null values\")\n",
    "print(' ')\n",
    "\n",
    "#removing rows with null values\n",
    "# dataset.dropna(inplace=True)\n",
    "\n",
    "# fill missing values with mean column values\n",
    "dataset.fillna(dataset.mean(), inplace=True)\n",
    "##############\n",
    "\n",
    "#Convert catagorical attributes to numeric values\n",
    "\n",
    "for col in dataset.columns.values:\n",
    "     if dataset[col].dtypes=='object':\n",
    "            \n",
    "        le.fit(dataset[col].values)\n",
    "        dataset[col]=le.transform(dataset[col])\n",
    "        \n",
    "\n",
    "###########################\n",
    "#Test and Train data split\n",
    "##########################\n",
    "\n",
    "\n",
    "\n",
    "def test_train(dataset,seed):\n",
    "    #Number of columns\n",
    "    n= len(dataset.columns)\n",
    "    array = dataset.values\n",
    "    X = array[:,0:n-1]\n",
    "    y = array[:,n-1]\n",
    "    validation_size = 0.20\n",
    "    X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, y, test_size=validation_size, random_state=seed)\n",
    "    return  X_train, X_validation, Y_train, Y_validation\n",
    "\n",
    "#Run the function\n",
    "X_train, X_validation, Y_train, Y_validation = test_train(dataset,seed)\n",
    "print(' ')\n",
    "print(\"Data split done\")\n",
    "print(' ')\n",
    "\n",
    "###########################################\n",
    "#Preprocessing data before modeling\n",
    "#We dp data standerization. Standardization is a way to deal with these values that lie so far apart\n",
    "#0 and standard deviation of 1\n",
    "###########################################\n",
    "\n",
    "def standard(X_train,X_test):\n",
    "# Define the scaler \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    # Scale the train set\n",
    "    X_train = scaler.transform(X_train)\n",
    "\n",
    "    # Scale the test set\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "#################################\n",
    "#Building models for classification\n",
    "#################################\n",
    "\n",
    "def crossval(X_train, Y_train,seed):\n",
    "    models= []\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeClassifier()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('SVM', SVC()))\n",
    "\n",
    "    results=[]\n",
    "    names =[]\n",
    "    scoring = 'accuracy'\n",
    "    result_mean ={}\n",
    "    \n",
    "    for name, model in models:\n",
    "        kfold=model_selection.KFold(n_splits=10, random_state=seed)\n",
    "        cv_result= model_selection.cross_val_score(model,X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "        results.append(cv_result)\n",
    "        names.append(name)\n",
    "        msg = \"%s:%f (%f)\" % (name,cv_result.mean(), cv_result.std())\n",
    "        result_mean[name]=cv_result.mean()\n",
    "        #print(msg)\n",
    "\n",
    "    return results, names,result_mean\n",
    "        \n",
    "\n",
    "#Run the function\n",
    "results, names, result_mean=crossval(X_train, Y_train,seed)\n",
    "\n",
    "\n",
    "\n",
    "# print the best classifier\n",
    "max_key = max(result_mean, key=lambda k: result_mean[k])\n",
    "\n",
    "\n",
    "print(\"Cross validation done\")\n",
    "print(' ')\n",
    "###########################\n",
    "#Plotting results of cross_validation\n",
    "#########################\n",
    "\n",
    "# fig = plt.figure()\n",
    "# fig.suptitle('Algorithm Comparison')\n",
    "# ax= fig.add_subplot(111)\n",
    "# plt.boxplot(results)\n",
    "# ax.set_xticklabels(names)\n",
    "# plt.show()\n",
    "\n",
    "################################\n",
    "#Using the best classifier\n",
    "################################\n",
    "\n",
    "def best_class(max_key, X_train, Y_train, X_validation, Y_validation,seed):\n",
    "    \n",
    "     if(max_key)=='KNN':      \n",
    "\n",
    "        # Make predictions on validation dataset\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(X_train, Y_train)\n",
    "        predictions = knn.predict(X_validation)\n",
    "        \n",
    "            \n",
    "     elif(max_key)=='LR':\n",
    "        \n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(X_train, Y_train)\n",
    "        predictions = lr.predict(X_validation)\n",
    "        classifier =lr\n",
    "        \n",
    "        \n",
    "     elif(max_key)=='LDA':\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        lda.fit(X_train, Y_train)\n",
    "        predictions = lda.predict(X_validation)\n",
    "        classifier =lda\n",
    "\n",
    "     elif(max_key)=='CART':\n",
    "        cart= DecisionTreeClassifier()\n",
    "        cart.fit(X_train, Y_train)\n",
    "        predictions = cart.predict(X_validation)\n",
    "        classifier =cart\n",
    "\n",
    "     elif(max_key)=='NB':\n",
    "        nb= GaussianNB()\n",
    "        nb.fit(X_train, Y_train)\n",
    "        predictions = nb.predict(X_validation)\n",
    "        classifier =nb\n",
    "\n",
    "     elif(max_key)=='SVM':\n",
    "        svm= SVC()\n",
    "        svm.fit(X_train, Y_train)\n",
    "        predictions = svm.predict(X_validation)\n",
    "        classifier =svm\n",
    "    \n",
    "     else:\n",
    "            print('Error in model selection')\n",
    "    \n",
    "     #accuracy shows the number of observtions predicted correctly / total number of observations\n",
    "     accuracy=accuracy_score(Y_validation,predictions)\n",
    "    \n",
    "     #confusion matrix shows the corectly and incorrect predictions for each class. \n",
    "     confusion=confusion_matrix(Y_validation,predictions)\n",
    "    \n",
    "     #Precision is the  number of positive predictions divided by the total number of positive class values predicted\n",
    "     # Also called  Positive Predictive Value (PPV). Calculated as (TP)/(TP+FP)\n",
    "     #Recall (Sensitivity) number of positive predictions divided by the number of positive class values in\n",
    "     #the test data (Total cases). Calculated as (TP)/(TP+FN)\n",
    "     #The F1 Score is the 2*((precision*recall)/(precision+recall))\n",
    "     report = classification_report(Y_validation,predictions)\n",
    "    \n",
    "     return accuracy, confusion,report, classifier\n",
    "\n",
    "#Run the function\n",
    "accuracy, confusion,report,classifier= best_class(max_key, X_train, Y_train, X_validation, Y_validation,seed)\n",
    "\n",
    "print(\"Model fitting done\")\n",
    "print(' ')\n",
    "\n",
    "def results(accuracy, confusion,report,max_key):\n",
    "    print(\"The best classifier is\")\n",
    "    print(max_key)    \n",
    "    print(\"The accuracy of the model is: \")\n",
    "    print(accuracy*100)\n",
    "    print(' ')\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "    print(' ')\n",
    "    print(\"Classification Report: \")\n",
    "    print(report)\n",
    "    \n",
    "#Run function\n",
    "results(accuracy, confusion,report,max_key)\n",
    "\n",
    "####################################\n",
    "#Model into action\n",
    "###################################\n",
    "\n",
    "def classify_data(classifier, data):\n",
    "    print(\"According to the model the correct classification is:\")\n",
    "    print((classifier.predict(data)[0]))\n",
    "\n",
    "#Give the data here\n",
    "\n",
    "# data= [[6.7,3.1,5.6,2.4]]\n",
    "# data =[[50,5,83311,9 ,13,2,4,0,4,1,0,0,13,25]]\n",
    "\n",
    "#Here you go the results\n",
    "\n",
    "#classify_data(classifier, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal-length</th>\n",
       "      <th>sepal-width</th>\n",
       "      <th>petal-length</th>\n",
       "      <th>petal-width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal-length  sepal-width  petal-length  petal-width  class\n",
       "147           6.5          3.0           5.2          2.0      2\n",
       "148           6.2          3.4           5.4          2.3      2\n",
       "149           5.9          3.0           5.1          1.8      2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tail(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the model the correct classification is:\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "data=[[4.6 ,3.1, 1.5, 0.2   ]]\n",
    "classify_data(classifier, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
